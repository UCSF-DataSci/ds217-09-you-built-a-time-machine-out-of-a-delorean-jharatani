{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd9864b",
   "metadata": {},
   "source": [
    "# Question 2: Resampling and Frequency Conversion\n",
    "\n",
    "This question focuses on resampling operations and frequency conversion using ICU monitoring data (hourly) and patient vital signs data (daily).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d645281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a454b02",
   "metadata": {},
   "source": [
    "## Part 2.1: Load and Prepare Data\n",
    "\n",
    "**Note:** These datasets have realistic characteristics:\n",
    "- **ICU Monitoring**: 75 patients with variable stay lengths (2-30 days). Not all patients are present for the entire 6-month period - patients are admitted and discharged at different times.\n",
    "- **Patient Vitals**: Already contains some missing visits (~5% missing data). This is realistic and will be useful for practicing missing data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b166d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU monitoring shape: (86400, 7)\n",
      "Patient vitals shape: (18250, 7)\n",
      "\n",
      "ICU monitoring sample:\n",
      "                    patient_id  heart_rate  blood_pressure_systolic  \\\n",
      "datetime                                                              \n",
      "2023-01-01 00:00:00     ICU001   82.000000                      126   \n",
      "2023-01-01 01:00:00     ICU001   98.294095                      128   \n",
      "2023-01-01 02:00:00     ICU001  103.500000                      129   \n",
      "2023-01-01 03:00:00     ICU001   91.535534                      136   \n",
      "2023-01-01 04:00:00     ICU001   87.330127                      129   \n",
      "\n",
      "                     blood_pressure_diastolic  oxygen_saturation  temperature  \n",
      "datetime                                                                       \n",
      "2023-01-01 00:00:00                        65                 96    98.783988  \n",
      "2023-01-01 01:00:00                        67                 95    99.186212  \n",
      "2023-01-01 02:00:00                        68                 94    98.800638  \n",
      "2023-01-01 03:00:00                        75                 96    98.349004  \n",
      "2023-01-01 04:00:00                        68                 95    98.643958  \n",
      "\n",
      "Patient vitals sample:\n",
      "           patient_id  temperature  heart_rate  blood_pressure_systolic  \\\n",
      "date                                                                      \n",
      "2023-01-01      P0001    98.389672          71                      119   \n",
      "2023-01-02      P0001    98.492046          67                      117   \n",
      "2023-01-03      P0001    98.790163          70                      113   \n",
      "2023-01-04      P0001    98.635781          74                      117   \n",
      "2023-01-05      P0001    98.051660          67                      118   \n",
      "\n",
      "            blood_pressure_diastolic     weight  \n",
      "date                                             \n",
      "2023-01-01                        84  68.996865  \n",
      "2023-01-02                        82  67.720215  \n",
      "2023-01-03                        78  67.846825  \n",
      "2023-01-04                        82  67.693993  \n",
      "2023-01-05                        83  68.228852  \n",
      "\n",
      "ICU patients: 20\n",
      "ICU date range: 2023-01-01 00:00:00 to 2023-06-29 23:00:00\n",
      "\n",
      "Patient vitals patients: 50\n",
      "Patient vitals date range: 2023-01-01 00:00:00 to 2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load ICU monitoring data (hourly)\n",
    "icu_monitoring = pd.read_csv('data/icu_monitoring.csv')\n",
    "\n",
    "# Load patient vitals data (daily) - for comparison\n",
    "patient_vitals = pd.read_csv('data/patient_vitals.csv')\n",
    "\n",
    "print(\"ICU monitoring shape:\", icu_monitoring.shape)\n",
    "print(\"Patient vitals shape:\", patient_vitals.shape)\n",
    "\n",
    "# Convert datetime columns and set as index\n",
    "icu_monitoring['datetime'] = pd.to_datetime(icu_monitoring['datetime'])\n",
    "icu_monitoring = icu_monitoring.set_index('datetime')\n",
    "\n",
    "patient_vitals['date'] = pd.to_datetime(patient_vitals['date'])\n",
    "patient_vitals = patient_vitals.set_index('date')\n",
    "\n",
    "print(\"\\nICU monitoring sample:\")\n",
    "print(icu_monitoring.head())\n",
    "print(\"\\nPatient vitals sample:\")\n",
    "print(patient_vitals.head())\n",
    "\n",
    "# Check data characteristics\n",
    "print(f\"\\nICU patients: {icu_monitoring['patient_id'].nunique()}\")\n",
    "print(f\"ICU date range: {icu_monitoring.index.min()} to {icu_monitoring.index.max()}\")\n",
    "print(f\"\\nPatient vitals patients: {patient_vitals['patient_id'].nunique()}\")\n",
    "print(f\"Patient vitals date range: {patient_vitals.index.min()} to {patient_vitals.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7dd39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Part 2.2: Time Series Selection\n",
    "\n",
    "**‚ö†Ô∏è WARNING: Sort Index Before Date Selection!**\n",
    "Since multiple patients share the same date, the `patient_vitals` index is non-monotonic (not strictly increasing). **You MUST sort the index first** before using `.loc` with date ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f905616",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "patient_vitals = patient_vitals.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551db0cf",
   "metadata": {},
   "source": [
    "Without sorting, pandas cannot reliably handle date range selections and may return unexpected results or errors.\n",
    "\n",
    "**TODO: Perform time series indexing and selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8e2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 1, 2023 data:            patient_id  temperature  heart_rate  blood_pressure_systolic  \\\n",
      "date                                                                      \n",
      "2023-01-23      P0044    98.563784          61                      112   \n",
      "2023-01-23      P0002    99.239016          86                      105   \n",
      "2023-01-23      P0005    97.740030          57                      118   \n",
      "2023-01-23      P0043    98.727877          85                      110   \n",
      "2023-01-23      P0027    98.454538          93                      117   \n",
      "2023-01-23      P0014    98.212433          78                      118   \n",
      "2023-01-23      P0013    99.057357          71                      117   \n",
      "2023-01-23      P0017    99.239211          79                      112   \n",
      "2023-01-23      P0021    99.851922          76                      125   \n",
      "2023-01-23      P0050    99.456538          74                      125   \n",
      "2023-01-23      P0039    99.481110          93                      131   \n",
      "2023-01-23      P0019    98.502100          84                      121   \n",
      "2023-01-23      P0011    98.896723          78                      125   \n",
      "2023-01-23      P0006    99.031960          67                      116   \n",
      "2023-01-23      P0024    98.179897          64                      112   \n",
      "2023-01-23      P0009    99.054253          87                      124   \n",
      "2023-01-23      P0015    98.780779          76                      109   \n",
      "2023-01-23      P0033    98.320540          65                      106   \n",
      "2023-01-23      P0046    98.522881          83                      122   \n",
      "2023-01-23      P0018    98.471533          83                      126   \n",
      "2023-01-23      P0003    99.297490          87                      108   \n",
      "2023-01-23      P0030    99.082032          68                      122   \n",
      "2023-01-23      P0026    99.235477          78                      110   \n",
      "2023-01-23      P0048    98.504360          82                      122   \n",
      "2023-01-23      P0036    99.674210          88                      122   \n",
      "2023-01-23      P0007    99.229873          79                      131   \n",
      "2023-01-23      P0045    98.517713          66                      121   \n",
      "2023-01-23      P0023    99.371467          62                      114   \n",
      "2023-01-23      P0029    99.563809          65                      124   \n",
      "2023-01-23      P0037    99.605491          59                      112   \n",
      "2023-01-23      P0025    99.306699          86                      121   \n",
      "2023-01-23      P0031    98.653835          65                      120   \n",
      "2023-01-23      P0041    99.335493          86                      125   \n",
      "2023-01-23      P0034    98.306009          81                      128   \n",
      "2023-01-23      P0008    98.532900          85                      114   \n",
      "2023-01-23      P0038    98.191776          65                      128   \n",
      "2023-01-23      P0010    99.164811          89                      117   \n",
      "2023-01-23      P0042    98.525110          82                      123   \n",
      "2023-01-23      P0028    98.296961          72                      112   \n",
      "2023-01-23      P0016    98.843418          61                      112   \n",
      "2023-01-23      P0040    98.727520          75                      125   \n",
      "2023-01-23      P0032    98.836441          69                      124   \n",
      "2023-01-23      P0049    98.051915          58                      128   \n",
      "2023-01-23      P0020    98.936953          67                      114   \n",
      "2023-01-23      P0035    99.076684          70                      109   \n",
      "2023-01-23      P0004    99.179539          79                      129   \n",
      "2023-01-23      P0022    98.587348          69                      121   \n",
      "2023-01-23      P0012    98.842680          72                      128   \n",
      "2023-01-23      P0047    98.800347          63                      118   \n",
      "2023-01-23      P0001    98.749293          67                      118   \n",
      "\n",
      "            blood_pressure_diastolic      weight  \n",
      "date                                              \n",
      "2023-01-23                        67   72.072837  \n",
      "2023-01-23                        65   57.848243  \n",
      "2023-01-23                        85   86.477348  \n",
      "2023-01-23                        72   64.653678  \n",
      "2023-01-23                        72   73.779496  \n",
      "2023-01-23                        72   83.282420  \n",
      "2023-01-23                        88   59.232390  \n",
      "2023-01-23                        81   80.880422  \n",
      "2023-01-23                        73   78.203241  \n",
      "2023-01-23                        76   74.554143  \n",
      "2023-01-23                        80   58.500116  \n",
      "2023-01-23                        71   70.038460  \n",
      "2023-01-23                        81   61.458747  \n",
      "2023-01-23                        84   52.046152  \n",
      "2023-01-23                        71   49.354414  \n",
      "2023-01-23                        66   62.597722  \n",
      "2023-01-23                        69   65.368250  \n",
      "2023-01-23                        67   50.384269  \n",
      "2023-01-23                        65   62.969607  \n",
      "2023-01-23                        87   68.753311  \n",
      "2023-01-23                        68   79.403069  \n",
      "2023-01-23                        79   60.087193  \n",
      "2023-01-23                        71   48.632921  \n",
      "2023-01-23                        84   62.522256  \n",
      "2023-01-23                        79   80.384677  \n",
      "2023-01-23                        75   57.562062  \n",
      "2023-01-23                        75   76.722872  \n",
      "2023-01-23                        79   48.784918  \n",
      "2023-01-23                        74   59.653620  \n",
      "2023-01-23                        79   77.654630  \n",
      "2023-01-23                        68   84.168737  \n",
      "2023-01-23                        82   72.847095  \n",
      "2023-01-23                        78   50.989731  \n",
      "2023-01-23                        80   67.428545  \n",
      "2023-01-23                        71   69.205568  \n",
      "2023-01-23                        84   40.000000  \n",
      "2023-01-23                        71   58.607305  \n",
      "2023-01-23                        75   59.625334  \n",
      "2023-01-23                        81  104.268191  \n",
      "2023-01-23                        79   83.885162  \n",
      "2023-01-23                        79   80.152431  \n",
      "2023-01-23                        73   74.951837  \n",
      "2023-01-23                        81   49.043085  \n",
      "2023-01-23                        78   70.796992  \n",
      "2023-01-23                        72   77.006246  \n",
      "2023-01-23                        85   94.463656  \n",
      "2023-01-23                        78   69.136821  \n",
      "2023-01-23                        78   48.819733  \n",
      "2023-01-23                        84   55.071798  \n",
      "2023-01-23                        83   68.098509  \n",
      "Records on Jan 1: 50 (some patients may start later)\n",
      "January 2023 shape: (1550, 6)\n",
      "DatetimeIndex(['2023-01-01', '2023-04-01', '2023-07-01', '2023-10-01'], dtype='datetime64[ns]', freq='QS-JAN')\n",
      "\n",
      "First quarter average temperature: 98.97¬∞F\n",
      "After June average temperature: 98.41¬∞F\n",
      "First week average temperature: 98.68¬∞F\n",
      "Last week average temperature: 98.65¬∞F\n",
      "Business hours data shape: (32400, 6)\n",
      "\n",
      "Average heart rate - All hours: 81.7 bpm\n",
      "Average heart rate - Business hours: 80.7 bpm\n",
      "Average temperature - All hours: 98.5¬∞F\n",
      "Average temperature - Business hours: 98.5¬∞F\n"
     ]
    }
   ],
   "source": [
    "# TODO: Select data by specific dates\n",
    "# Note: Not all patients may have data on January 1, 2023 (some start later)\n",
    "# Important: Sort the index first since multiple patients share the same date\n",
    "# patient_vitals = patient_vitals.sort_index()  # Sort for reliable date-based selection\n",
    "january_first = patient_vitals.loc[\"2023-01-23\"]  # Select January 1, 2023 from patient_vitals\n",
    "print(\"January 1, 2023 data:\", january_first)\n",
    "print(f\"Records on Jan 1: {len(january_first)} (some patients may start later)\")\n",
    "\n",
    "# TODO: Select data by date ranges\n",
    "january_data = patient_vitals.loc[\"2023-01-01\":\"2023-01-31\"]\n",
    "print(\"January 2023 shape:\", january_data.shape)\n",
    "\n",
    "# TODO: Select data by time periods\n",
    "quarterly = pd.date_range('2023-01-01','2023-12-31', freq = 'QS')\n",
    "print(quarterly)\n",
    "first_quarter = patient_vitals.loc['2023-01-01':'2023-04-01'] #select Q1 2023\n",
    "entire_year = patient_vitals.loc['2023']  # Select all of 2023 (will include patients with partial year data)\n",
    "\n",
    "# TODO: Select first and last periods using .loc\n",
    "first_week = patient_vitals.loc[:patient_vitals.index.min() + pd.Timedelta(days=6)]  # First 7 days\n",
    "last_week = patient_vitals.loc[patient_vitals.index.max() - pd.Timedelta(days=6):]  # Last 7 days\n",
    "\n",
    "# TODO: Use truncate() method\n",
    "# Note: truncate() requires a sorted index. Sort first if needed: patient_vitals = patient_vitals.sort_index()\n",
    "data_after_june = patient_vitals.truncate(before='2023-06-01')  # Truncate before June 1, 2023\n",
    "data_before_september = patient_vitals.truncate(after='2023-08-31')  # Truncate after August 31, 2023\n",
    "\n",
    "# TODO: Use selected data for analysis\n",
    "# Compare average temperature between first quarter and data after June\n",
    "print(f\"\\nFirst quarter average temperature: {first_quarter['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"After June average temperature: {data_after_june['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"First week average temperature: {first_week['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"Last week average temperature: {last_week['temperature'].mean():.2f}¬∞F\")\n",
    "\n",
    "# For ICU data with time components:\n",
    "# TODO: Select business hours (9 AM to 5 PM)\n",
    "business_hours = icu_monitoring.between_time('9:00','17:00')  # Use between_time()\n",
    "print(\"Business hours data shape:\", business_hours.shape)\n",
    "\n",
    "# TODO: Select specific time (noon readings)\n",
    "noon_data = icu_monitoring.at_time('12:00')  # Use at_time('12:00')\n",
    "\n",
    "# TODO: Use time-based selection for analysis\n",
    "# Compare vital signs during business hours vs other times\n",
    "all_hours_avg = icu_monitoring.select_dtypes(include=[np.number]).mean()\n",
    "business_hours_avg = business_hours.select_dtypes(include=[np.number]).mean()\n",
    "print(f\"\\nAverage heart rate - All hours: {all_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average heart rate - Business hours: {business_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average temperature - All hours: {all_hours_avg['temperature']:.1f}¬∞F\")\n",
    "print(f\"Average temperature - Business hours: {business_hours_avg['temperature']:.1f}¬∞F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f2fe",
   "metadata": {},
   "source": [
    "## Part 2.3: Resampling Operations\n",
    "\n",
    "**TODO: Perform resampling and frequency conversion**\n",
    "\n",
    "**Important Note:** When resampling DataFrames that contain non-numeric columns (like `patient_id`), you'll get an error if you try to aggregate them with numeric functions like `mean()`. Use `df.select_dtypes(include=[np.number])` to select only numeric columns before resampling, or specify which columns to aggregate in `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18466a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU daily shape: (180, 5)\n",
      "Weekly resampled shape: (53, 5)\n",
      "Monthly resampled shape: (12, 5)\n",
      "Missing values after upsampling: temperature                 323\n",
      "heart_rate                  323\n",
      "blood_pressure_systolic     323\n",
      "blood_pressure_diastolic    323\n",
      "weight                      323\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Resample hourly ICU data to daily\n",
    "# Note: Exclude non-numeric columns like 'patient_id' when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols = icu_monitoring.select_dtypes(include=[np.number]).columns\n",
    "icu_daily = icu_monitoring[numeric_cols].resample('D').mean()\n",
    "print(\"ICU daily shape:\", icu_daily.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to weekly\n",
    "# Note: Exclude 'patient_id' column when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols_pv = patient_vitals.select_dtypes(include=[np.number]).columns\n",
    "patient_vitals_weekly = patient_vitals[numeric_cols_pv].resample('W').mean()\n",
    "print(\"Weekly resampled shape:\", patient_vitals_weekly.shape)\n",
    "\n",
    "# TODO: Resample daily patient data to monthly\n",
    "patient_vitals_monthly = patient_vitals[numeric_cols_pv].resample('ME').mean()  # Resample to monthly with mean aggregation (use freq='ME' for Month End)\n",
    "print(\"Monthly resampled shape:\", patient_vitals_monthly.shape)\n",
    "\n",
    "# TODO: Use different aggregation functions (mean, sum, max, min)\n",
    "icu_daily_stats = icu_monitoring.resample('D').agg({'heart_rate': ['mean', 'max', 'min'], \n",
    "                         'temperature': 'mean'})  # Resample with multiple aggregations\n",
    "\n",
    "\n",
    "# TODO: Handle missing values during resampling\n",
    "# Demonstrate upsampling (monthly to daily) creates missing values\n",
    "# Note: When upsampling, use .asfreq() to create missing values, or use .resample() with aggregation\n",
    "monthly_to_daily = patient_vitals_monthly.asfreq('D')  # Upsample monthly data to daily (use .asfreq() or .resample('D'))\n",
    "print(\"Missing values after upsampling:\", monthly_to_daily.isna().sum())\n",
    "\n",
    "# TODO: Compare different resampling frequencies\n",
    "# Create a DataFrame comparing resampling results at different frequencies\n",
    "# Important: Since patient_vitals contains multiple patients per date, you need to aggregate by date first\n",
    "# to create a single daily time series for comparison.\n",
    "# Why aggregation is needed: The patient_vitals DataFrame has multiple rows per date (one for each patient),\n",
    "# so we need to average across patients for each date to create a single daily time series that can be\n",
    "# meaningfully compared with the weekly and monthly resampled data. Without aggregation, resampling would\n",
    "# operate on each patient's time series separately, making it difficult to compare frequencies meaningfully.\n",
    "# Steps:\n",
    "# 1. Since 'date' is currently the index, reset it to a column first, then aggregate by date\n",
    "#    Note: groupby('date').mean() automatically sets 'date' as the index in the result, so you don't need\n",
    "#    to call set_index('date') again after groupby.\n",
    "patient_vitals_reset = patient_vitals[numeric_cols_pv].reset_index()\n",
    "patient_vitals_daily_agg = patient_vitals_reset.groupby('date').mean()\n",
    "#    # The date is already the index after groupby, so no need to set_index again\n",
    "# 2. Compare the aggregated daily data with weekly and monthly resampled data\n",
    "# Use patient_vitals data resampled to different frequencies:\n",
    "# - Original daily data (aggregated by date): patient_vitals_daily_agg\n",
    "# - Weekly resampled (patient_vitals_weekly) \n",
    "# - Monthly resampled (patient_vitals_monthly)\n",
    "# Include columns: frequency, date_range, row_count, mean_temperature, std_temperature\n",
    "# Use the 'temperature' column from each resampled dataset\n",
    "# Example structure:\n",
    "resampling_comparison = pd.DataFrame({\n",
    "     'frequency': ['daily', 'weekly', 'monthly'],\n",
    "     'date_range': [\n",
    "         f\"{patient_vitals_daily_agg.index.min().date()} - {patient_vitals_daily_agg.index.max().date()}\",\n",
    "         f\"{patient_vitals_weekly.index.min().date()} - {patient_vitals_weekly.index.max().date()}\",\n",
    "         f\"{patient_vitals_monthly.index.min().date()} - {patient_vitals_monthly.index.max().date()}\"],  # Use index.min() and index.max() for each dataset\n",
    "     'row_count': [\n",
    "         len(patient_vitals_daily_agg),\n",
    "         len(patient_vitals_weekly),\n",
    "         len(patient_vitals_monthly)\n",
    "     ],  # Use len() for each dataset\n",
    "     'mean_temperature': [patient_vitals_daily_agg['temperature'].mean(),\n",
    "        patient_vitals_weekly['temperature'].mean(),\n",
    "        patient_vitals_monthly['temperature'].mean()],  # Use .mean() on 'temperature' column for each dataset\n",
    "     'std_temperature': [patient_vitals_daily_agg['temperature'].std(),\n",
    "        patient_vitals_weekly['temperature'].std(),\n",
    "        patient_vitals_monthly['temperature'].std()]   # Use .std() on 'temperature' column for each dataset\n",
    " })\n",
    "\n",
    "# TODO: Save results as 'output/q2_resampling_analysis.csv'\n",
    "resampling_comparison.to_csv('output/q2_resampling_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605486d",
   "metadata": {},
   "source": [
    "## Part 2.4: Missing Data Handling\n",
    "\n",
    "**üí° TIP: High Percentage of Missing Data is Expected!**\n",
    "When upsampling from monthly to daily frequency, you'll create approximately 96% missing data (only 12 month-end dates have values out of 365 days). This is normal and expected for upsampling - don't be alarmed!\n",
    "\n",
    "**Approach:** Create missing values by upsampling monthly data to daily frequency. This creates a clear, structured pattern of missing data that's ideal for practicing imputation methods.\n",
    "\n",
    "**TODO: Handle missing data in time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dd87022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value count: 323\n",
      "Missing value percentage: 96.41791044776119\n",
      "Missing value count after time-based interpolation: 0\n",
      "Missing value percentage after time-based interpolation: 0.0\n",
      "Missing value count after rolling mean imputation: 0\n",
      "Missing value percentage afetr rolling mean imputation: 0.0\n",
      "date\n",
      "2023-01-31    98.777000\n",
      "2023-02-01          NaN\n",
      "2023-02-02          NaN\n",
      "2023-02-03          NaN\n",
      "2023-02-04          NaN\n",
      "                ...    \n",
      "2023-12-27          NaN\n",
      "2023-12-28          NaN\n",
      "2023-12-29          NaN\n",
      "2023-12-30          NaN\n",
      "2023-12-31    98.538865\n",
      "Freq: D, Name: temperature, Length: 335, dtype: float64\n",
      "date\n",
      "2023-01-31    98.777000\n",
      "2023-02-01    98.785011\n",
      "2023-02-02    98.793021\n",
      "2023-02-03    98.801031\n",
      "2023-02-04    98.809041\n",
      "                ...    \n",
      "2023-12-27    98.508585\n",
      "2023-12-28    98.516155\n",
      "2023-12-29    98.523725\n",
      "2023-12-30    98.531295\n",
      "2023-12-31    98.538865\n",
      "Freq: D, Name: temperature, Length: 335, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Identify missing values in time series\n",
    "# Use the monthly resampled data from Part 2.3 and upsample to daily:\n",
    "#   - Take patient_vitals_monthly['temperature']\n",
    "#   - Upsample to daily frequency using .resample('D').asfreq()\n",
    "#   - This creates missing values for all days except month-end dates (~96% missing)\n",
    "ts_with_missing = patient_vitals_monthly['temperature'].resample('D').asfreq()  # Time series with missing values\n",
    "print(\"Missing value count:\", ts_with_missing.isna().sum())\n",
    "print(\"Missing value percentage:\", ts_with_missing.isna().sum() / len(ts_with_missing) * 100)\n",
    "\n",
    "# TODO: Use forward fill and backward fill\n",
    "ts_ffill = ts_with_missing.ffill()  # Forward fill missing values (use .ffill() method)\n",
    "ts_bfill = ts_with_missing.bfill()  # Backward fill missing values (use .bfill() method)\n",
    "\n",
    "# TODO: Use interpolation methods\n",
    "ts_interpolated = ts_with_missing.interpolate()  # Interpolate missing values\n",
    "ts_interpolated_linear = ts_with_missing.interpolate(method='linear')  # Linear interpolation\n",
    "ts_interpolated_time = ts_with_missing.interpolate(method='time')  # Time-based interpolation\n",
    "print(\"Missing value count after time-based interpolation:\", ts_interpolated_time.isna().sum())\n",
    "print(\"Missing value percentage after time-based interpolation:\", ts_interpolated_time.isna().sum()/len(ts_interpolated_time) *100)\n",
    "\n",
    "# TODO: Use rolling mean for imputation\n",
    "ts_roll = ts_ffill.rolling(window=5, min_periods = 1).mean() # Have to use ffill data because this takes care of the trailing missing values, just using rolling mean doesn't work\n",
    "ts_rolling_imputed = ts_ffill.where(ts_ffill.notna(), ts_roll)\n",
    "print(\"Missing value count after rolling mean imputation:\", ts_rolling_imputed.isna().sum())\n",
    "print(\"Missing value percentage afetr rolling mean imputation:\", ts_rolling_imputed.isna().sum()/len(ts_rolling_imputed) *100)\n",
    " # Fill missing with rolling mean\n",
    "\n",
    "print(ts_with_missing)\n",
    "print(ts_interpolated_time)\n",
    "\n",
    "# TODO: Create missing data report\n",
    "# Document your missing data handling with the following sections:\n",
    "# 1. Missing value summary: Total count and percentage\n",
    "# 2. Missing data patterns: When/why data is missing (by month, day of week, etc.)\n",
    "# 3. Imputation method: Which method you used (forward fill, backward fill, interpolation, rolling mean)\n",
    "# 4. Rationale: Why you chose that method\n",
    "# 5. Pros and cons: Advantages and limitations of your approach\n",
    "# 6. Example: Show at least one example of missing data before and after imputation\n",
    "# Minimum length: 300 words\n",
    "missing_data_report = \"\"\"TODO: Document your missing data handling:\n",
    "- Missing value summary: There were 323 missing values found, about 96.42 percent of the data was missing.\n",
    "This was among multiple columns, most missing values were found in heart rate, blood pressure, and weight.\n",
    "- Missing data patterns: We used multiple imputation methods including forward fill, backward fill,\n",
    "     interpolation, linear interpolation, time-base interpolation and rolling mean imputation. My preference was towards the time-base interpolation based on the percentage of the data missing,\n",
    "     and my understanding of interpolation using time as a third variable to help with imputation of missing values. Another method I was leaning towards using was a more basic forward fill, \n",
    "     however, because there is so much data missing, I am hesitant to use this method as it may cause overestimation of values that are not actually reflective of the true data.\n",
    "- Rationale: I believe this to be a more reliable way to impute data.\n",
    "     Using the wrong imputation method can lead to negative outcomes when more data is missing such as extreme skews or incorrect representations of the overall trends.\n",
    "- Pros and Cons: Some pros with time-base interpolation is that the data will not stray too far from the original data points and we can use this in a larger dataset as a way \n",
    "to keep consistency with trends that may be observed. However, the same risks apply as with any form of imputation where if you overfit a dataset to a trend you observe, i.e. you \n",
    "impute too many datasets, you can lead to something disastrous like creating your own data based off of minimal data points that are collected. So, in general, it is always safer to \n",
    "be more conservative with imputation methods no matter how precise they may be. You can also use more than one method to keep variation and avoid some form of overfitting or over-imputation \n",
    "if you will.\n",
    "- Example:\n",
    "date\n",
    "2023-01-31    98.777000\n",
    "2023-02-01          NaN\n",
    "2023-02-02          NaN\n",
    "2023-02-03          NaN\n",
    "2023-02-04          NaN\n",
    "                ...    \n",
    "2023-12-27          NaN\n",
    "2023-12-28          NaN\n",
    "2023-12-29          NaN\n",
    "2023-12-30          NaN\n",
    "2023-12-31    98.538865\n",
    "Freq: D, Name: temperature, Length: 335, dtype: float64\n",
    "date\n",
    "2023-01-31    98.777000\n",
    "2023-02-01    98.785011\n",
    "2023-02-02    98.793021\n",
    "2023-02-03    98.801031\n",
    "2023-02-04    98.809041\n",
    "...\n",
    "2023-12-29    98.523725\n",
    "2023-12-30    98.531295\n",
    "2023-12-31    98.538865\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Document missing data patterns\n",
    "# Analyze when/why data is missing\n",
    "missing_by_month = ts_with_missing.groupby(ts_with_missing.index.month).apply(lambda x: x.isna().sum())\n",
    "missing_by_day = ts_with_missing.groupby(ts_with_missing.index.dayofweek).apply(lambda x: x.isna().sum())\n",
    "missing_patterns = f\"Missing by month:\\n{missing_by_month}\\n\\nMissing by day of week:\\n{missing_by_day}\"\n",
    "\n",
    "# TODO: Save results as 'output/q2_missing_data_report.txt'\n",
    "with open('output/q2_missing_data_report.txt', 'w') as f:\n",
    "     f.write(missing_data_report)\n",
    "     f.write(f\"\\n\\nMissing patterns:\\n{missing_patterns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf350d",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before moving to Question 3, verify you've created:\n",
    "\n",
    "- [ ] `output/q2_resampling_analysis.csv` - resampling analysis results\n",
    "- [ ] `output/q2_missing_data_report.txt` - missing data handling report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
